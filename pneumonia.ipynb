{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install keras tensorflow-gpu cudatoolkit cudnn\n",
    "!pip3 install matplotlib\n",
    "!pip3 install numpy\n",
    "!pip3 install pandas\n",
    "!pip3 install scikit-learn\n",
    "!pip3 install seaborn\n",
    "!pip3 install 'tensorflow[and-cuda]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121, Xception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gpu():\n",
    "\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Number of TensorFlow GPUs: {len(gpus)}\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return device\n",
    "device = setup_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_generators(data_dir='chest_xray', img_size=(224, 224), batch_size=32):\n",
    "\n",
    "    train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        f'{data_dir}/train',\n",
    "        seed=123,\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='binary'\n",
    "    )\n",
    "    \n",
    "    val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        f'{data_dir}/val',\n",
    "        seed=123,\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='binary'\n",
    "    )\n",
    "    \n",
    "    test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        f'{data_dir}/test',\n",
    "        seed=123,\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='binary'\n",
    "    )\n",
    "    \n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomRotation(0.2),\n",
    "        tf.keras.layers.RandomZoom(0.2),\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomContrast(0.2)\n",
    "    ])\n",
    "    \n",
    "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "    \n",
    "    # Apply augmentation and normalization to training data\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda x, y: (data_augmentation(x), y)\n",
    "    ).map(\n",
    "        lambda x, y: (normalization_layer(x), y)\n",
    "    )\n",
    "    \n",
    "    # Apply only normalization to validation and test data\n",
    "    val_dataset = val_dataset.map(\n",
    "        lambda x, y: (normalization_layer(x), y)\n",
    "    )\n",
    "    test_dataset = test_dataset.map(\n",
    "        lambda x, y: (normalization_layer(x), y)\n",
    "    )\n",
    "    \n",
    "    # Configure datasets for performance\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"chest_xray/train\"\n",
    "\n",
    "filepath =[]\n",
    "label = []\n",
    "\n",
    "folds = os.listdir(directory)\n",
    "\n",
    "for fold in folds:\n",
    "    f_path = os.path.join(directory , fold)\n",
    "    \n",
    "    imgs = os.listdir(f_path)\n",
    "    \n",
    "    for img in imgs:\n",
    "        \n",
    "        img_path = os.path.join(f_path , img)\n",
    "        filepath.append(img_path)\n",
    "        label.append(fold)\n",
    "        \n",
    "#Concat data paths with labels\n",
    "file_path_series = pd.Series(filepath , name= 'filepath')\n",
    "Label_path_series = pd.Series(label , name = 'label')\n",
    "df_train = pd.concat([file_path_series ,Label_path_series ] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df_train['label'].value_counts()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6), facecolor='white')\n",
    "\n",
    "palette = sns.color_palette(\"viridis\")\n",
    "sns.set_palette(palette)\n",
    "axs[0].pie(count, labels=count.index, autopct='%1.1f%%', startangle=140)\n",
    "axs[0].set_title('Distribution of Categories')\n",
    "\n",
    "sns.barplot(x=count.index, y=count.values, ax=axs[1], palette=\"viridis\")\n",
    "axs[1].set_title('Count of Categories')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"chest_xray/test\"\n",
    "\n",
    "filepath =[]\n",
    "label = []\n",
    "\n",
    "folds = os.listdir(directory)\n",
    "\n",
    "for fold in folds:\n",
    "    f_path = os.path.join(directory , fold)\n",
    "    \n",
    "    imgs = os.listdir(f_path)\n",
    "    \n",
    "    for img in imgs:\n",
    "        \n",
    "        img_path = os.path.join(f_path , img)\n",
    "        filepath.append(img_path)\n",
    "        label.append(fold)\n",
    "        \n",
    "#Concat data paths with labels\n",
    "file_path_series = pd.Series(filepath , name= 'filepath')\n",
    "Label_path_series = pd.Series(label , name = 'label')\n",
    "df_test = pd.concat([file_path_series ,Label_path_series ] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df_test['label'].value_counts()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6), facecolor='white')\n",
    "\n",
    "palette = sns.color_palette(\"viridis\")\n",
    "sns.set_palette(palette)\n",
    "axs[0].pie(count, labels=count.index, autopct='%1.1f%%', startangle=140)\n",
    "axs[0].set_title('Distribution of Categories')\n",
    "\n",
    "sns.barplot(x=count.index, y=count.values, ax=axs[1], palette=\"viridis\")\n",
    "axs[1].set_title('Count of Categories')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_images(path, num_images=5):\n",
    "    \n",
    "    image_filenames = os.listdir(path)\n",
    "    \n",
    "    num_images = min(num_images, len(image_filenames))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3),facecolor='white')\n",
    "    \n",
    "    for i, image_filename in enumerate(image_filenames[:num_images]):\n",
    "        image_path = os.path.join(path, image_filename)\n",
    "        image = mpimg.imread(image_path)\n",
    "        \n",
    "        axes[i].imshow(image)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(image_filename)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_visualize = \"chest_xray/train/NORMAL\"\n",
    "visualize_images(path_to_visualize, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_visualize = \"chest_xray/train/PNEUMONIA\"\n",
    "visualize_images(path_to_visualize, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xception_model():\n",
    "    base_model = Xception(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.45),\n",
    "        tf.keras.layers.Dense(220, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Dense(60,activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_cnn():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(224, 224, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_transfer_learning_model(base_model_name='VGG16'):\n",
    "\n",
    "    if base_model_name == 'VGG16':\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    elif base_model_name == 'ResNet50':\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    elif base_model_name == 'DenseNet121':\n",
    "        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "        for layer in base_model.layers[-20:]:\n",
    "            layer.trainable = True\n",
    "    elif base_model_name == 'CNN':\n",
    "        base_model = create_custom_cnn()\n",
    "        return base_model\n",
    "    elif base_model_name == 'Xception':\n",
    "        base_model = create_xception_model()\n",
    "        return base_model\n",
    "    if base_model_name == 'VGG16' or base_model_name == 'ResNet50' or base_model_name == 'CNN':\n",
    "        model = tf.keras.Sequential([\n",
    "            base_model,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "    elif base_model_name == 'DenseNet121':\n",
    "        model = tf.keras.Sequential([\n",
    "            base_model,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model(model_name):\n",
    "    \"\"\"Load a saved model\"\"\"\n",
    "    try:\n",
    "        return tf.keras.models.load_model(f'best_{model_name}.keras')\n",
    "    except (OSError, IOError) as e:\n",
    "        print(\"Model file not found, continuining with training...\")\n",
    "    return None\n",
    "\n",
    "def evaluate_model(model, test_dataset):\n",
    "    \"\"\"Evaluate a model on test data\"\"\"\n",
    "    test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "    print(f\"\\nTest accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test loss: {test_loss:.4f}\")\n",
    "    \n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path):\n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = img_array / 255.0  # Normalize pixel values\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(img_array)\n",
    "    \n",
    "    # For binary classification\n",
    "    result = \"Pneumonia\" if prediction[0] > 0.5 else \"Normal\"\n",
    "    confidence = prediction[0] if prediction[0] > 0.5 else 1 - prediction[0]\n",
    "    \n",
    "    return result, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, model_name, continue_training=False):\n",
    "\n",
    "    model_path = f'best_{model_name}.h5'\n",
    "\n",
    "    if not continue_training and os.path.exists(model_path):\n",
    "        print(f\"Loading existing model: {model_path}\")\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "    else:\n",
    "        print(\"Creating new model...\")\n",
    "        model = create_transfer_learning_model(model_name)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.Recall(), tf.keras.metrics.F1Score()]\n",
    "    )\n",
    "\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "        patience=5,  # Increased patience\n",
    "        restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            min_lr=0.00001\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            f'best_{model_name}.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_f1_score'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=20,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    print(history)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    best_epoch = results['val_accuracy'].index(max(results['val_accuracy'])) + 1\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    axs[0].plot(results['accuracy'], label='Training Accuracy', color='blue')\n",
    "    axs[0].plot(results['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "    axs[0].scatter(best_epoch - 1, results['val_accuracy'][best_epoch - 1], color='green', label=f'Best Epoch: {best_epoch}')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_title('Training and Validation Accuracy')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    axs[1].plot(results['loss'], label='Training Loss', color='blue')\n",
    "    axs[1].plot(results['val_loss'], label='Validation Loss', color='red')\n",
    "    axs[1].scatter(best_epoch - 1, results['val_loss'][best_epoch - 1], color='green',label=f'Best Epoch: {best_epoch}')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_title('Training and Validation Loss')\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def create_balanced_test_set(source_dir, dest_dir, n_samples=20):\n",
    "    os.makedirs(os.path.join(dest_dir, 'normal'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dest_dir, 'pneumonia'), exist_ok=True)\n",
    "    \n",
    "    for class_name in ['NORMAL', 'PNEUMONIA']:\n",
    "        source_files = os.listdir(os.path.join(source_dir, class_name))[:n_samples]\n",
    "        for file_name in source_files:\n",
    "            shutil.copy(\n",
    "                os.path.join(source_dir, class_name, file_name),\n",
    "                os.path.join(dest_dir, class_name, file_name)\n",
    "            )\n",
    "create_balanced_test_set('chest_xray/test/','chest_xray/balanced-test/')\n",
    "\n",
    "test_dir = 'chest_xray/balanced-test/'\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir, \n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=40,\n",
    "    shuffle=True,\n",
    "    validation_split=None,\n",
    "    class_names=['normal', 'pneumonia'])\n",
    "class_labels = ['normal', 'pneumonia']\n",
    "def plot_images_with_predictions(model, dataset, class_labels, num_images=40):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for img, lbl in dataset.take(1):\n",
    "        images = img.numpy()\n",
    "        labels = lbl.numpy()\n",
    "    \n",
    "    predictions = model.predict(dataset)\n",
    "    predictions = (predictions > 0.5).astype(int)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(min(num_images, len(images))):\n",
    "        plt.subplot(8, 5, i + 1)\n",
    "        plt.imshow(images[i].astype(\"uint8\"))\n",
    "        true_class = class_labels[int(labels[i])]\n",
    "        pred_class = class_labels[int(predictions[i])]\n",
    "        plt.title(f'True: {true_class}\\nPred: {pred_class}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "cnn_best = load_saved_model('VGG16')\n",
    "plot_images_with_predictions(cnn_best, test_ds, class_labels)\n",
    "cnn_best = load_saved_model('CNN')\n",
    "plot_images_with_predictions(cnn_best, test_ds, class_labels)\n",
    "cnn_best = load_saved_model('ResNet50')\n",
    "plot_images_with_predictions(cnn_best, test_ds, class_labels)\n",
    "cnn_best = load_saved_model('DenseNet121')\n",
    "plot_images_with_predictions(cnn_best, test_ds, class_labels)\n",
    "cnn_best = load_saved_model('Xception')\n",
    "plot_images_with_predictions(cnn_best, test_ds, class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "def compare_models():\n",
    "    models = {\n",
    "        'CNN': create_custom_cnn(),\n",
    "        'VGG16': create_transfer_learning_model('VGG16'),\n",
    "        'ResNet50': create_transfer_learning_model('ResNet50'),\n",
    "        'DenseNet121': create_transfer_learning_model('DenseNet121'),\n",
    "        'Xception': create_xception_model()\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        \n",
    "        if name == \"DenseNet121\":\n",
    "            policy = mixed_precision.Policy('mixed_float16')\n",
    "            mixed_precision.set_global_policy(policy)\n",
    "\n",
    "        history = train_and_evaluate(model, name)\n",
    "\n",
    "        if history.history != None:\n",
    "            results = history.history\n",
    "            plot_results(results)\n",
    "            results = None\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_comparison(metrics):\n",
    "    # Bar plot for Recall and F1 Score\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    models = list(metrics.keys())\n",
    "    recalls = [m['recall'] for m in metrics.values()]\n",
    "    f1_scores = [m['f1'] for m in metrics.values()]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, recalls, width, label='Recall')\n",
    "    plt.bar(x + width/2, f1_scores, width, label='F1 Score')\n",
    "    \n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.xticks(x, models, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(metrics):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    fig.suptitle('Confusion Matrices for Different Models')\n",
    "    \n",
    "    for (name, metric), ax in zip(metrics.items(), axes.flat):\n",
    "        sns.heatmap(metric['confusion_matrix'], \n",
    "                   annot=True, \n",
    "                   fmt='d',\n",
    "                   cmap='Blues',\n",
    "                   ax=ax)\n",
    "        ax.set_title(name)\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('True')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_performance_table(metrics):\n",
    "    data = {\n",
    "        'Model': [],\n",
    "        'Recall': [],\n",
    "        'F1 Score': []\n",
    "    }\n",
    "    \n",
    "    for name, metric in metrics.items():\n",
    "        data['Model'].append(name)\n",
    "        data['Recall'].append(f\"{metric['recall']:.4f}\")\n",
    "        data['F1 Score'].append(f\"{metric['f1']:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, recall_score, f1_score\n",
    "\n",
    "def load_and_evaluate_models(test_ds):\n",
    "    metrics = {}\n",
    "    \n",
    "    model_names = ['ResNet50', 'VGG16', 'DenseNet121', 'CNN', 'Xception']\n",
    "    \n",
    "    for name in model_names:\n",
    "        model = load_saved_model(name)\n",
    "        \n",
    "        y_pred = model.predict(test_ds)\n",
    "        y_pred = (y_pred > 0.5).astype(int)\n",
    "        \n",
    "        y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "        \n",
    "        metrics[name] = {\n",
    "            'recall': recall_score(y_true, y_pred),\n",
    "            'f1': f1_score(y_true, y_pred),\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred)\n",
    "        }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"Running code on GPU:\", gpus)\n",
    "else:\n",
    "    print(\"Running code on CPU:\", tf.config.list_physical_devices('CPU'))\n",
    "\n",
    "train_ds, val_ds, test_ds = create_data_generators()\n",
    "\n",
    "compare_models()\n",
    "metrics = load_and_evaluate_models(test_ds)\n",
    "\n",
    "plot_performance_comparison(metrics)\n",
    "plot_confusion_matrices(metrics)\n",
    "\n",
    "performance_table = create_performance_table(metrics)\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(performance_table)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
